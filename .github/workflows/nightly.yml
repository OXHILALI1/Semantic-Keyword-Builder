name: Nightly Health Check

on:
  schedule:
    # Run every night at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  comprehensive-health-check:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov || echo "pytest not available, using unittest"

    - name: Run comprehensive test suite
      run: |
        echo "Running nightly comprehensive test suite..."
        python run_tests.py

    - name: Repository health check
      run: |
        echo "Checking repository health..."
        
        # Count workflow files
        if [ -d "workflows" ]; then
          workflow_count=$(find workflows -name "*.json" | wc -l)
          echo "Total workflow files: $workflow_count"
          
          # Check for any corrupted JSON files
          corrupted_count=0
          for workflow in workflows/*.json; do
            if [ -f "$workflow" ]; then
              if ! python -c "import json; json.load(open('$workflow'))" 2>/dev/null; then
                echo "‚ö†Ô∏è Corrupted workflow file: $workflow"
                corrupted_count=$((corrupted_count + 1))
              fi
            fi
          done
          
          echo "Corrupted workflow files: $corrupted_count"
          
          if [ $corrupted_count -gt 0 ]; then
            echo "‚ùå Repository has corrupted workflow files"
            exit 1
          else
            echo "‚úÖ All workflow files are valid JSON"
          fi
        else
          echo "No workflows directory found"
        fi

    - name: Performance benchmark
      run: |
        echo "Running performance benchmarks..."
        
        python -c "
import time
import tempfile
import json
import statistics
from new_workflow_analyzer import NewWorkflowAnalyzer
from auto_add_workflow import AutoWorkflowAdder

# Create test workflow
test_workflow = {
    'id': 'benchmark-test',
    'name': 'Benchmark Test Workflow',
    'nodes': [
        {'id': 'node1', 'type': 'n8n-nodes-base.manualTrigger'},
        {'id': 'node2', 'type': 'n8n-nodes-base.slack'},
        {'id': 'node3', 'type': 'n8n-nodes-base.gmail'}
    ],
    'connections': {'node1': {'main': [['node2']]}, 'node2': {'main': [['node3']]}},
    'active': True
}

with tempfile.TemporaryDirectory() as temp_dir:
    # Create test file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, dir=temp_dir) as f:
        json.dump(test_workflow, f)
        temp_file = f.name
    
    # Benchmark analysis
    analyzer = NewWorkflowAnalyzer(temp_dir)
    analysis_times = []
    
    for i in range(20):
        start_time = time.time()
        result = analyzer.analyze_workflow_file(temp_file)
        end_time = time.time()
        
        if result and result.get('success'):
            analysis_times.append(end_time - start_time)
        else:
            print(f'‚ùå Analysis failed on iteration {i}')
            exit(1)
    
    avg_analysis_time = statistics.mean(analysis_times)
    max_analysis_time = max(analysis_times)
    min_analysis_time = min(analysis_times)
    
    print(f'Analysis Performance:')
    print(f'  Average: {avg_analysis_time:.3f}s')
    print(f'  Min: {min_analysis_time:.3f}s')
    print(f'  Max: {max_analysis_time:.3f}s')
    
    # Benchmark addition (dry run)
    adder = AutoWorkflowAdder(temp_dir)
    addition_times = []
    
    for i in range(10):
        start_time = time.time()
        result = adder.add_workflow(temp_file, auto_confirm=True, dry_run=True)
        end_time = time.time()
        
        if result and result.get('success'):
            addition_times.append(end_time - start_time)
        else:
            print(f'‚ùå Addition failed on iteration {i}')
            exit(1)
    
    avg_addition_time = statistics.mean(addition_times)
    print(f'Addition Performance:')
    print(f'  Average: {avg_addition_time:.3f}s')
    
    # Performance thresholds
    if avg_analysis_time > 0.5:
        print('‚ö†Ô∏è Analysis performance degraded (>0.5s average)')
    else:
        print('‚úÖ Analysis performance good')
    
    if avg_addition_time > 1.0:
        print('‚ö†Ô∏è Addition performance degraded (>1.0s average)')
    else:
        print('‚úÖ Addition performance good')
        "

    - name: Memory usage check
      run: |
        echo "Checking memory usage..."
        
        python -c "
import psutil
import tempfile
import json
from new_workflow_analyzer import NewWorkflowAnalyzer

# Create large test workflow
large_workflow = {
    'id': 'memory-test',
    'name': 'Memory Test Workflow',
    'nodes': [],
    'connections': {},
    'active': True
}

# Add many nodes
for i in range(1000):
    node = {
        'id': f'node-{i:04d}',
        'name': f'Node {i}',
        'type': 'n8n-nodes-base.manualTrigger',
        'position': [i * 10, (i % 10) * 100],
        'parameters': {'description': f'Test node {i} with some data'}
    }
    large_workflow['nodes'].append(node)

with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
    json.dump(large_workflow, f)
    temp_file = f.name

try:
    # Measure memory before
    process = psutil.Process()
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # Analyze large workflow
    analyzer = NewWorkflowAnalyzer()
    result = analyzer.analyze_workflow_file(temp_file)
    
    # Measure memory after
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_increase = memory_after - memory_before
    
    print(f'Memory usage:')
    print(f'  Before: {memory_before:.1f} MB')
    print(f'  After: {memory_after:.1f} MB')
    print(f'  Increase: {memory_increase:.1f} MB')
    
    if memory_increase > 100:  # More than 100MB increase
        print('‚ö†Ô∏è High memory usage detected')
    else:
        print('‚úÖ Memory usage within acceptable limits')
    
    if result and result.get('success'):
        print('‚úÖ Large workflow analysis successful')
    else:
        print('‚ùå Large workflow analysis failed')
        exit(1)
        
finally:
    import os
    os.unlink(temp_file)
        "

    - name: Integration health check
      run: |
        echo "Checking integration health..."
        
        # Test that all test files can be imported
        python -c "
import sys
import os
sys.path.insert(0, 'tests')

test_modules = [
    'tests.unit.test_workflow_analyzer',
    'tests.unit.test_auto_add_workflow',
    'tests.integration.test_api_endpoints',
    'tests.integration.test_file_operations',
    'tests.e2e.test_complete_workflow'
]

for module in test_modules:
    try:
        __import__(module)
        print(f'‚úÖ {module}')
    except ImportError as e:
        print(f'‚ùå {module}: {e}')
        exit(1)

print('‚úÖ All test modules can be imported')
        "

    - name: Generate health report
      run: |
        echo "Generating nightly health report..."
        
        {
          echo "# Nightly Health Report - $(date)"
          echo ""
          echo "## Test Results"
          echo "- Comprehensive test suite: ‚úÖ PASSED"
          echo "- Repository health: ‚úÖ PASSED"
          echo "- Performance benchmarks: ‚úÖ PASSED"
          echo "- Memory usage: ‚úÖ PASSED"
          echo "- Integration health: ‚úÖ PASSED"
          echo ""
          echo "## Repository Statistics"
          if [ -d "workflows" ]; then
            echo "- Total workflow files: $(find workflows -name '*.json' | wc -l)"
          else
            echo "- Total workflow files: 0 (no workflows directory)"
          fi
          echo "- Test files: $(find tests -name 'test_*.py' | wc -l)"
          echo "- Core modules: $(ls *.py | grep -E '(analyzer|adder|test)' | wc -l)"
          echo ""
          echo "Generated at: $(date -u) UTC"
        } > nightly_report.md
        
        cat nightly_report.md

    - name: Cleanup and summary
      run: |
        echo ""
        echo "üåô Nightly health check completed successfully!"
        echo "Repository is healthy and all systems are operational."

  dependency-check:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Check for dependency updates
      run: |
        echo "Checking Python standard library compatibility..."
        
        # Test with different Python versions (simulate)
        python_versions=("3.8" "3.9" "3.10" "3.11")
        
        for version in "${python_versions[@]}"; do
          echo "Checking compatibility with Python $version..."
          
          # Check for version-specific issues
          if [ "$version" = "3.8" ]; then
            # Check for walrus operator usage (3.8+)
            if grep -r ":=" --include="*.py" .; then
              echo "‚ö†Ô∏è Walrus operator found - ensure Python 3.8+ compatibility"
            fi
          fi
          
          if [ "$version" = "3.9" ]; then
            # Check for dict union operator (3.9+)
            if grep -r "|\s*{" --include="*.py" .; then
              echo "‚ö†Ô∏è Dict union operator found - ensure Python 3.9+ compatibility"
            fi
          fi
        done
        
        echo "‚úÖ Basic compatibility check completed"

    - name: Security vulnerability scan
      run: |
        echo "Running basic security checks..."
        
        # Check for eval/exec usage
        if grep -r -E "(eval|exec)\s*\(" --include="*.py" .; then
          echo "‚ö†Ô∏è Found eval/exec usage - review for security implications"
        else
          echo "‚úÖ No eval/exec usage found"
        fi
        
        # Check for subprocess with shell=True
        if grep -r "shell\s*=\s*True" --include="*.py" .; then
          echo "‚ö†Ô∏è Found subprocess with shell=True - review for security implications"
        else
          echo "‚úÖ No shell=True subprocess calls found"
        fi